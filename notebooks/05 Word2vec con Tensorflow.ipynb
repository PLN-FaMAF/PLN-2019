{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec with Tensorflow\n",
    "\n",
    "https://adventuresinmachinelearning.com/word2vec-tutorial-tensorflow/\n",
    "\n",
    "https://github.com/adventuresinML/adventures-in-ml-code/blob/master/tf_word2vec.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $ head -10000 uba.txt > uba-10000.txt\n",
    "filename = '/home/francolq/tass2018/uba-10000.txt'\n",
    "with open(filename) as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = tf.compat.as_str(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "def build_dataset(words, n_words):\n",
    "    \"\"\"Process raw inputs into a dataset.\"\"\"\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(n_words - 1))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0  # dictionary['UNK']\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reversed_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = data2.split()\n",
    "n_words = 10000\n",
    "data, count, d, rd = build_dataset(words, n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mañana', 'me', 'voy', 'a', 'mi', 'casa', 'con', '10', 'kilos', 'más']\n",
      "[172, 8, 46, 5, 27, 184, 22, 262, 5736, 89]\n",
      "[172, 8, 46, 5, 27, 184, 22, 262, 5736, 89]\n",
      "['Mañana', 'me', 'voy', 'a', 'mi', 'casa', 'con', '10', 'kilos', 'más']\n"
     ]
    }
   ],
   "source": [
    "print(words[:10])\n",
    "print([d[w] for w in words[:10]])\n",
    "print(data[:10])\n",
    "print([rd[i] for i in data[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import collections\n",
    "\n",
    "data_index = 0\n",
    "# generate batch data\n",
    "def generate_batch(data, batch_size, num_skips, skip_window):\n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    context = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1  # [ skip_window input_word skip_window ]\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    for i in range(batch_size // num_skips):\n",
    "        target = skip_window  # input word at the center of the buffer\n",
    "        targets_to_avoid = [skip_window]\n",
    "        for j in range(num_skips):\n",
    "            while target in targets_to_avoid:\n",
    "                target = random.randint(0, span - 1)\n",
    "            targets_to_avoid.append(target)\n",
    "            batch[i * num_skips + j] = buffer[skip_window]  # this is the input word\n",
    "            context[i * num_skips + j, 0] = buffer[target]  # these are the context words\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    # Backtrack a little bit to avoid skipping words in the end of a batch\n",
    "    data_index = (data_index + len(data) - span) % len(data)\n",
    "    return batch, context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "skip_window = 1       # How many words to consider left and right.\n",
    "num_skips = 2\n",
    "batch, context = generate_batch(data, batch_size, num_skips, skip_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['me', 'me', 'voy', 'voy', 'a', 'a', 'mi', 'mi', 'casa', 'casa', 'con', 'con', '10', '10', 'kilos', 'kilos', 'más', 'más', '😒', '😒']\n",
      "['voy', 'Mañana', 'a', 'me', 'voy', 'mi', 'a', 'casa', 'mi', 'con', '10', 'casa', 'con', 'kilos', 'más', '10', '😒', 'kilos', '😂', 'más']\n"
     ]
    }
   ],
   "source": [
    "print([rd[i] for i in batch[:20]])\n",
    "print([rd[i] for i in context[:20,0]])\n",
    "#context[:,0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, batch will be the input (the center tokens), and context will be the output (the left and right contexts that must be predicted).\n",
    "\n",
    "**Observations:**\n",
    "- First token is not sampled. And last?\n",
    "- Tweets are all together in a single sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model: step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 128  # Dimension of the embedding vector.\n",
    "train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "train_context = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "# (here it wrongly said train_labels in the tutorial)\n",
    "# valid_dataset = tf.constant(valid_examples, dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/francolq/.virtualenvs/pln2019/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# Look up embeddings for inputs.\n",
    "vocabulary_size = n_words  # 10000\n",
    "embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Construct the variables for the softmax\n",
    "weights = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                          stddev=1.0 / math.sqrt(embedding_size)))\n",
    "biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "hidden_out = tf.matmul(embed, tf.transpose(weights)) + biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this, we have:\n",
    "\n",
    "- E (embed): vocab x dim\n",
    "- W (weights): vocab x dim\n",
    "- b (biases): vocab\n",
    "\n",
    "out = E W^T + b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-13-e2a133c0ea47>:4: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "WARNING:tensorflow:From /home/francolq/.virtualenvs/pln2019/lib/python3.7/site-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "# convert train_context to a one-hot format\n",
    "train_one_hot = tf.one_hot(train_context, vocabulary_size)\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hidden_out, \n",
    "    labels=train_one_hot))\n",
    "# Construct the SGD optimizer using a learning rate of 1.0.\n",
    "optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**And this i don't know, but it looks like it is only for validation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-14-7295003c8465>:2: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'valid_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-7295003c8465>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnormalized_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membeddings\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m valid_embeddings = tf.nn.embedding_lookup(\n\u001b[0;32m----> 5\u001b[0;31m       normalized_embeddings, valid_dataset)\n\u001b[0m\u001b[1;32m      6\u001b[0m similarity = tf.matmul(\n\u001b[1;32m      7\u001b[0m       valid_embeddings, normalized_embeddings, transpose_b=True)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'valid_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# Compute the cosine similarity between minibatch examples and all embeddings.\n",
    "norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "normalized_embeddings = embeddings / norm\n",
    "valid_embeddings = tf.nn.embedding_lookup(\n",
    "      normalized_embeddings, valid_dataset)\n",
    "similarity = tf.matmul(\n",
    "      valid_embeddings, normalized_embeddings, transpose_b=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    embedding_size = 128  # Dimension of the embedding vector.\n",
    "    train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_context = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "    \n",
    "    vocabulary_size = n_words  # 10000\n",
    "    embeddings = tf.Variable(\n",
    "        tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "\n",
    "    # Construct the variables for the softmax\n",
    "    weights = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                              stddev=1.0 / math.sqrt(embedding_size)))\n",
    "    biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    hidden_out = tf.matmul(embed, tf.transpose(weights)) + biases\n",
    "\n",
    "    # convert train_context to a one-hot format\n",
    "    train_one_hot = tf.one_hot(train_context, vocabulary_size)\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hidden_out, \n",
    "        labels=train_one_hot))\n",
    "    # Construct the SGD optimizer using a learning rate of 1.0.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(cross_entropy)\n",
    "\n",
    "    init = tf.global_variables_initializer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 100\n",
    "# init = tf.global_variables_initializer()  # already done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step  0 :  9.347723007202148\n",
      "Average loss at step  1 :  0.004658615112304688\n",
      "Average loss at step  2 :  0.004629480838775635\n",
      "Average loss at step  3 :  0.00458070182800293\n",
      "Average loss at step  4 :  0.004641133308410645\n",
      "Average loss at step  5 :  0.004632509231567382\n",
      "Average loss at step  6 :  0.004548101425170898\n",
      "Average loss at step  7 :  0.004597013950347901\n",
      "Average loss at step  8 :  0.0046786203384399415\n",
      "Average loss at step  9 :  0.0046365785598754886\n",
      "Average loss at step  10 :  0.004614619731903076\n",
      "Average loss at step  11 :  0.004452562808990478\n",
      "Average loss at step  12 :  0.0046217765808105465\n",
      "Average loss at step  13 :  0.004466467380523682\n",
      "Average loss at step  14 :  0.004539928436279297\n",
      "Average loss at step  15 :  0.004534406661987305\n",
      "Average loss at step  16 :  0.004396549224853516\n",
      "Average loss at step  17 :  0.004445652961730957\n",
      "Average loss at step  18 :  0.004428196907043457\n",
      "Average loss at step  19 :  0.00458828067779541\n",
      "Average loss at step  20 :  0.004530688762664795\n",
      "Average loss at step  21 :  0.004449521064758301\n",
      "Average loss at step  22 :  0.004534523963928222\n",
      "Average loss at step  23 :  0.004529353618621826\n",
      "Average loss at step  24 :  0.004455339908599853\n",
      "Average loss at step  25 :  0.004309774398803711\n",
      "Average loss at step  26 :  0.004425704002380371\n",
      "Average loss at step  27 :  0.004490788459777832\n",
      "Average loss at step  28 :  0.004441351890563965\n",
      "Average loss at step  29 :  0.0044428391456604\n",
      "Average loss at step  30 :  0.004411913871765137\n",
      "Average loss at step  31 :  0.0043674116134643556\n",
      "Average loss at step  32 :  0.004346563339233399\n",
      "Average loss at step  33 :  0.004523913383483887\n",
      "Average loss at step  34 :  0.004367753028869629\n",
      "Average loss at step  35 :  0.004454550743103028\n",
      "Average loss at step  36 :  0.0044208292961120605\n",
      "Average loss at step  37 :  0.0045276470184326175\n",
      "Average loss at step  38 :  0.004194029808044434\n",
      "Average loss at step  39 :  0.004302892684936523\n",
      "Average loss at step  40 :  0.004330891609191895\n",
      "Average loss at step  41 :  0.004390239715576172\n",
      "Average loss at step  42 :  0.004175210952758789\n",
      "Average loss at step  43 :  0.0043992319107055665\n",
      "Average loss at step  44 :  0.00439472770690918\n",
      "Average loss at step  45 :  0.004521933555603028\n",
      "Average loss at step  46 :  0.004459868431091309\n",
      "Average loss at step  47 :  0.004507234573364258\n",
      "Average loss at step  48 :  0.0044312944412231445\n",
      "Average loss at step  49 :  0.004348523616790771\n",
      "Average loss at step  50 :  0.004404335975646973\n",
      "Average loss at step  51 :  0.004512349605560303\n",
      "Average loss at step  52 :  0.004509093284606934\n",
      "Average loss at step  53 :  0.004516259670257568\n",
      "Average loss at step  54 :  0.004497309207916259\n",
      "Average loss at step  55 :  0.004594878196716308\n",
      "Average loss at step  56 :  0.004466599464416504\n",
      "Average loss at step  57 :  0.004415850639343261\n",
      "Average loss at step  58 :  0.0044040136337280275\n",
      "Average loss at step  59 :  0.00448059606552124\n",
      "Average loss at step  60 :  0.004479548454284668\n",
      "Average loss at step  61 :  0.004364246368408203\n",
      "Average loss at step  62 :  0.004438720703125\n",
      "Average loss at step  63 :  0.004385268211364746\n",
      "Average loss at step  64 :  0.004432856559753418\n",
      "Average loss at step  65 :  0.004461268424987793\n",
      "Average loss at step  66 :  0.004364104270935059\n",
      "Average loss at step  67 :  0.0043947196006774904\n",
      "Average loss at step  68 :  0.0043017354011535645\n",
      "Average loss at step  69 :  0.004435319900512696\n",
      "Average loss at step  70 :  0.00429655647277832\n",
      "Average loss at step  71 :  0.004302962303161621\n",
      "Average loss at step  72 :  0.004295838832855225\n",
      "Average loss at step  73 :  0.004275031566619873\n",
      "Average loss at step  74 :  0.0043962116241455075\n",
      "Average loss at step  75 :  0.00433559513092041\n",
      "Average loss at step  76 :  0.004235212802886963\n",
      "Average loss at step  77 :  0.004292419910430908\n",
      "Average loss at step  78 :  0.004330520153045654\n",
      "Average loss at step  79 :  0.004307247161865234\n",
      "Average loss at step  80 :  0.004256317615509033\n",
      "Average loss at step  81 :  0.004342670440673828\n",
      "Average loss at step  82 :  0.004196001529693604\n",
      "Average loss at step  83 :  0.004326292037963867\n",
      "Average loss at step  84 :  0.004197877407073974\n",
      "Average loss at step  85 :  0.004226810455322265\n",
      "Average loss at step  86 :  0.004344965934753418\n",
      "Average loss at step  87 :  0.004367852210998535\n",
      "Average loss at step  88 :  0.004351803302764893\n",
      "Average loss at step  89 :  0.004315461158752441\n",
      "Average loss at step  90 :  0.004388413906097412\n",
      "Average loss at step  91 :  0.0041603479385375975\n",
      "Average loss at step  92 :  0.004100180625915527\n",
      "Average loss at step  93 :  0.004423470497131348\n",
      "Average loss at step  94 :  0.0043188157081604\n",
      "Average loss at step  95 :  0.00441461706161499\n",
      "Average loss at step  96 :  0.004204957008361816\n",
      "Average loss at step  97 :  0.0039543967247009275\n",
      "Average loss at step  98 :  0.004311703681945801\n",
      "Average loss at step  99 :  0.004331057548522949\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "  # We must initialize all variables before we use them.\n",
    "  init.run()\n",
    "  print('Initialized')\n",
    "\n",
    "  average_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batch_inputs, batch_context = generate_batch(data,\n",
    "        batch_size, num_skips, skip_window)\n",
    "    feed_dict = {train_inputs: batch_inputs, train_context: batch_context}\n",
    "\n",
    "    # We perform one update step by evaluating the optimizer op (including it\n",
    "    # in the list of returned values for session.run()\n",
    "    _, loss_val = session.run([optimizer, cross_entropy], feed_dict=feed_dict)\n",
    "    average_loss += loss_val\n",
    "\n",
    "    #if step % 2000 == 0:\n",
    "    if True:\n",
    "      if step > 0:\n",
    "        average_loss /= 2000\n",
    "      # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "      print('Average loss at step ', step, ': ', average_loss)\n",
    "      average_loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model with Negative Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sampled = 64    # Number of negative examples to sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "with graph.as_default():\n",
    "    # Construct the variables for the NCE loss\n",
    "    nce_weights = tf.Variable(\n",
    "        tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                            stddev=1.0 / math.sqrt(embedding_size)))\n",
    "    nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    nce_loss = tf.reduce_mean(\n",
    "        tf.nn.nce_loss(weights=nce_weights,\n",
    "                       biases=nce_biases,\n",
    "                       labels=train_context,\n",
    "                       inputs=embed,\n",
    "                       num_sampled=num_sampled,\n",
    "                       num_classes=vocabulary_size))\n",
    "    optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(nce_loss)\n",
    "\n",
    "    init = tf.global_variables_initializer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here:\n",
    "\n",
    "- nce_weights: vocab x dim\n",
    "- nce_biases: vocab\n",
    "- loss = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step  0 :  9.3038330078125\n",
      "Average loss at step  1 :  0.004653340339660645\n",
      "Average loss at step  2 :  0.0046351137161254885\n",
      "Average loss at step  3 :  0.0046837892532348635\n",
      "Average loss at step  4 :  0.004674358367919922\n",
      "Average loss at step  5 :  0.004685054779052735\n",
      "Average loss at step  6 :  0.0047127151489257816\n",
      "Average loss at step  7 :  0.004659594058990479\n",
      "Average loss at step  8 :  0.004666184425354004\n",
      "Average loss at step  9 :  0.004641903877258301\n",
      "Average loss at step  10 :  0.004634509563446045\n",
      "Average loss at step  11 :  0.004647600173950195\n",
      "Average loss at step  12 :  0.004657378196716308\n",
      "Average loss at step  13 :  0.00465419340133667\n",
      "Average loss at step  14 :  0.004656907081604004\n",
      "Average loss at step  15 :  0.004678123474121094\n",
      "Average loss at step  16 :  0.004656991958618164\n",
      "Average loss at step  17 :  0.004664521217346191\n",
      "Average loss at step  18 :  0.0046551403999328615\n",
      "Average loss at step  19 :  0.004666850090026855\n",
      "Average loss at step  20 :  0.004655470848083496\n",
      "Average loss at step  21 :  0.004678220272064209\n",
      "Average loss at step  22 :  0.00465354061126709\n",
      "Average loss at step  23 :  0.004632690906524658\n",
      "Average loss at step  24 :  0.004681822776794434\n",
      "Average loss at step  25 :  0.004675783157348633\n",
      "Average loss at step  26 :  0.004709144592285156\n",
      "Average loss at step  27 :  0.004672876834869385\n",
      "Average loss at step  28 :  0.004652285575866699\n",
      "Average loss at step  29 :  0.004587910652160645\n",
      "Average loss at step  30 :  0.004682902336120605\n",
      "Average loss at step  31 :  0.004637495994567871\n",
      "Average loss at step  32 :  0.004673887729644775\n",
      "Average loss at step  33 :  0.004654159545898437\n",
      "Average loss at step  34 :  0.004671092987060547\n",
      "Average loss at step  35 :  0.004651125431060791\n",
      "Average loss at step  36 :  0.004666528224945068\n",
      "Average loss at step  37 :  0.004683845520019532\n",
      "Average loss at step  38 :  0.0046335020065307615\n",
      "Average loss at step  39 :  0.0046594772338867186\n",
      "Average loss at step  40 :  0.004674287796020507\n",
      "Average loss at step  41 :  0.004682600975036621\n",
      "Average loss at step  42 :  0.0046877355575561525\n",
      "Average loss at step  43 :  0.00469247055053711\n",
      "Average loss at step  44 :  0.004695744514465332\n",
      "Average loss at step  45 :  0.004706357002258301\n",
      "Average loss at step  46 :  0.004696222305297851\n",
      "Average loss at step  47 :  0.004628241539001465\n",
      "Average loss at step  48 :  0.004653880119323731\n",
      "Average loss at step  49 :  0.004692933082580566\n",
      "Average loss at step  50 :  0.004642448425292969\n",
      "Average loss at step  51 :  0.0046680855751037595\n",
      "Average loss at step  52 :  0.004675797462463379\n",
      "Average loss at step  53 :  0.004662925243377685\n",
      "Average loss at step  54 :  0.0046653270721435545\n",
      "Average loss at step  55 :  0.004692353248596191\n",
      "Average loss at step  56 :  0.004699505805969238\n",
      "Average loss at step  57 :  0.00464962100982666\n",
      "Average loss at step  58 :  0.0047103786468505856\n",
      "Average loss at step  59 :  0.004649481773376465\n",
      "Average loss at step  60 :  0.004698834896087647\n",
      "Average loss at step  61 :  0.004675020694732666\n",
      "Average loss at step  62 :  0.004708970069885254\n",
      "Average loss at step  63 :  0.004699832916259765\n",
      "Average loss at step  64 :  0.004671391487121582\n",
      "Average loss at step  65 :  0.004713619709014892\n",
      "Average loss at step  66 :  0.004686976432800293\n",
      "Average loss at step  67 :  0.004639652729034424\n",
      "Average loss at step  68 :  0.004674906730651855\n",
      "Average loss at step  69 :  0.004722316741943359\n",
      "Average loss at step  70 :  0.00471164083480835\n",
      "Average loss at step  71 :  0.004684199810028076\n",
      "Average loss at step  72 :  0.004692039489746094\n",
      "Average loss at step  73 :  0.004687891006469726\n",
      "Average loss at step  74 :  0.00471241569519043\n",
      "Average loss at step  75 :  0.004682987689971924\n",
      "Average loss at step  76 :  0.0046709938049316405\n",
      "Average loss at step  77 :  0.004679836273193359\n",
      "Average loss at step  78 :  0.004695087432861328\n",
      "Average loss at step  79 :  0.004675458908081054\n",
      "Average loss at step  80 :  0.004664948463439941\n",
      "Average loss at step  81 :  0.004707076072692871\n",
      "Average loss at step  82 :  0.004659873962402344\n",
      "Average loss at step  83 :  0.004697414398193359\n",
      "Average loss at step  84 :  0.004691747665405274\n",
      "Average loss at step  85 :  0.004681020736694336\n",
      "Average loss at step  86 :  0.004645815849304199\n",
      "Average loss at step  87 :  0.0047319307327270506\n",
      "Average loss at step  88 :  0.0046640710830688475\n",
      "Average loss at step  89 :  0.004700631141662598\n",
      "Average loss at step  90 :  0.004684405326843262\n",
      "Average loss at step  91 :  0.00467458438873291\n",
      "Average loss at step  92 :  0.004674614429473877\n",
      "Average loss at step  93 :  0.004698857307434082\n",
      "Average loss at step  94 :  0.004725751399993897\n",
      "Average loss at step  95 :  0.004660204410552978\n",
      "Average loss at step  96 :  0.004685262680053711\n",
      "Average loss at step  97 :  0.004675772666931153\n",
      "Average loss at step  98 :  0.0046840858459472655\n",
      "Average loss at step  99 :  0.004698824882507324\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "  # We must initialize all variables before we use them.\n",
    "  init.run()\n",
    "  print('Initialized')\n",
    "\n",
    "  average_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batch_inputs, batch_context = generate_batch(data,\n",
    "        batch_size, num_skips, skip_window)\n",
    "    feed_dict = {train_inputs: batch_inputs, train_context: batch_context}\n",
    "\n",
    "    # We perform one update step by evaluating the optimizer op (including it\n",
    "    # in the list of returned values for session.run()\n",
    "    _, loss_val = session.run([optimizer, cross_entropy], feed_dict=feed_dict)\n",
    "    average_loss += loss_val\n",
    "\n",
    "    #if step % 2000 == 0:\n",
    "    if True:\n",
    "      if step > 0:\n",
    "        average_loss /= 2000\n",
    "      # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "      print('Average loss at step ', step, ': ', average_loss)\n",
    "      average_loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
