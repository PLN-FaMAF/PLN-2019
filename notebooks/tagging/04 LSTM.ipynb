{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM\n",
    "\n",
    "PoS tagger del castellano basado en LTSMs. Entrenado con el corpus Ancora.\n",
    "\n",
    "Basado en:\n",
    "\n",
    "- https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html#example-an-lstm-for-part-of-speech-tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f52a1ca3c50>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tagging.ancora import SimpleAncoraCorpusReader\n",
    "\n",
    "# load training data\n",
    "files = 'CESS-CAST-(A|AA|P)/.*\\.tbf\\.xml'\n",
    "corpus = SimpleAncoraCorpusReader('ancora-3.0.1es/', files)\n",
    "train_sents = list(corpus.tagged_sents())\n",
    "train_sents = [s for s in train_sents if s]  # remove empty sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test data\n",
    "test_files = '3LB-CAST/.*\\.tbf\\.xml'\n",
    "test_corpus = SimpleAncoraCorpusReader('ancora-3.0.1es/', test_files)\n",
    "test_sents = list(test_corpus.tagged_sents())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map to Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "wcount = Counter()\n",
    "tagset = set()\n",
    "for sent in train_sents:\n",
    "    words, tags = zip(*sent)\n",
    "    wcount.update(words)\n",
    "    tagset.update(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39393, 83)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wcount), len(tagset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000  # including the unknown element\n",
    "tagset_size = len(tagset)\n",
    "vocab = sorted(set(w for w, _ in wcount.most_common(vocab_size - 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_to_i = dict((v, i + 1) for i, v in enumerate(vocab))\n",
    "v_to_i['xxxunk'] = 0  # unknown element is mapped to 0\n",
    "t_to_i = dict((v, i) for i, v in enumerate(sorted(tagset)))\n",
    "i_to_t = sorted(tagset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7540"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_to_i.get('pelota', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    # use 0 for unknown words\n",
    "    idxs = [to_ix.get(w, 0) for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4714, 5664,    0, 7648])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = prepare_sequence('el gato come pescado'.split(), v_to_i)\n",
    "sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 10\n",
    "hidden_dim = 10\n",
    "model = LSTMTagger(embedding_dim, hidden_dim, vocab_size, tagset_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tag a Single Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent, tags = zip(*test_sents[0])\n",
    "seq = prepare_sequence(sent, v_to_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('El presidente del órgano regulador de las Telecomunicaciones se mostró partidario de completar esta liberalización de las telecomunicaciones con otras medidas que incentiven la competencia como_puede_ser abrir el acceso a la información de los clientes de Telefónica a otros operadores .',\n",
       " tensor([ 763, 7939, 4161, 9987,    0, 4039, 6443,    0, 8854, 6948, 7467, 4039,\n",
       "         3442, 5102, 6500, 4039, 6443, 9342, 3492, 7364, 6774, 8248,    0, 6412,\n",
       "         3426,    0, 1965, 4714, 1989, 1885, 6412, 6110, 4039, 6613, 3273, 4039,\n",
       "         1727, 1885, 7366, 7297,    8]))"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(sent), seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    tag_scores = model(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([41])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = tag_scores.argmax(dim=1)\n",
    "tags.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tags = [i_to_t[tag] for tag in tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('El', 'fs'), ('presidente', 'fe'), ('del', 'fe'), ('órgano', 'fe'), ('regulador', 'fs'), ('de', 'fe'), ('las', 'fe'), ('Telecomunicaciones', 'fs'), ('se', 'fe'), ('mostró', 'fs'), ('partidario', 'cs'), ('de', 'fe'), ('completar', 'p0000000'), ('esta', 'vag0000'), ('liberalización', 'dd0000'), ('de', 'fe'), ('las', 'fe'), ('telecomunicaciones', 'fs'), ('con', 'rn'), ('otras', 'fe'), ('medidas', 'fe'), ('que', 'fe'), ('incentiven', 'fs'), ('la', 'fe'), ('competencia', 'fs'), ('como_puede_ser', 'fs'), ('abrir', 'dd0000'), ('el', 'fs'), ('acceso', 'vag0000'), ('a', 'vag0000'), ('la', 'fs'), ('información', 'dd0000'), ('de', 'fe'), ('los', 'dd0000'), ('clientes', 'dd0000'), ('de', 'vaii000'), ('Telefónica', 'fe'), ('a', 'vag0000'), ('otros', 'fs'), ('operadores', 'fs'), ('.', 'fs')]\n"
     ]
    }
   ],
   "source": [
    "print(list(zip(sent, pred_tags)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative Log-Likelihood Loss\n",
    "# https://pytorch.org/docs/stable/nn.html#nllloss\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_sent = train_sents[0]\n",
    "sent, tags = zip(*tagged_sent)\n",
    "isent = prepare_sequence(sent, v_to_i)\n",
    "itags = prepare_sequence(tags, t_to_i)\n",
    "\n",
    "tag_scores = model(isent)  # forwad step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss_function(tag_scores, itags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.3652, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()  # calls backwards on tag_scores and then inside all the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.3399, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check new loss\n",
    "tag_scores = model(isent)\n",
    "loss = loss_function(tag_scores, itags)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13857"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at 999: 2.8925788106918335\n",
      "loss at 1999: 2.210839986205101\n",
      "loss at 2999: 1.9667545999288558\n",
      "loss at 3999: 1.7658093155026435\n",
      "loss at 4999: 1.704954649090767\n",
      "loss at 5999: 1.6245890661478042\n",
      "loss at 6999: 1.671179015338421\n",
      "loss at 7999: 1.6694490669369697\n",
      "loss at 8999: 1.5900325691998005\n",
      "loss at 9999: 1.5400521922707557\n",
      "loss at 10999: 1.463077837049961\n",
      "loss at 11999: 1.3678464286103844\n",
      "loss at 12999: 1.3661414576172828\n"
     ]
    }
   ],
   "source": [
    "# with a bit from https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "running_loss = 0.0\n",
    "for i, tagged_sent in enumerate(train_sents):\n",
    "    # optimizer.zero_grad()\n",
    "    model.zero_grad()  # are these equivalent?\n",
    "\n",
    "    sent, tags = zip(*tagged_sent)\n",
    "    isent = prepare_sequence(sent, v_to_i)\n",
    "    itags = prepare_sequence(tags, t_to_i)\n",
    "\n",
    "    tag_scores = model(isent)  # forwad step\n",
    "    loss = loss_function(tag_scores, itags)\n",
    "    loss.backward()  # calls backwards on tag_scores and then inside all the model\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    running_loss += loss.item()\n",
    "    if i % 1000 == 999:\n",
    "        print('loss at {}: {}'.format(i, running_loss / 1000))\n",
    "        running_loss = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at 1999: 1.2726990660130977\n",
      "loss at 3999: 1.2318624093532562\n"
     ]
    }
   ],
   "source": [
    "# with a bit from https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "\n",
    "for epoch in range(4):\n",
    "    running_loss = 0.0\n",
    "    for i, tagged_sent in enumerate(train_sents[:5000]):\n",
    "        # optimizer.zero_grad()\n",
    "        model.zero_grad()  # are these equivalent?\n",
    "\n",
    "        sent, tags = zip(*tagged_sent)\n",
    "        isent = prepare_sequence(sent, v_to_i)\n",
    "        itags = prepare_sequence(tags, t_to_i)\n",
    "\n",
    "        tag_scores = model(isent)  # forwad step\n",
    "        loss = loss_function(tag_scores, itags)\n",
    "        loss.backward()  # calls backwards on tag_scores and then inside all the model\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:\n",
    "            print('loss at {}: {}'.format(i, running_loss / 2000))\n",
    "            running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at 1999: 0.8682691221386194\n",
      "loss at 3999: 0.8556401647403836\n",
      "loss at 5999: 0.885220473356545\n",
      "loss at 7999: 0.9375973992832005\n",
      "loss at 9999: 0.9077403522077948\n",
      "loss at 11999: 0.8568295622328296\n",
      "loss at 1999: 0.8273824489563704\n",
      "loss at 3999: 0.8153675747662783\n",
      "loss at 5999: 0.8469382223002613\n",
      "loss at 7999: 0.895628368742764\n",
      "loss at 9999: 0.8662434305520729\n",
      "loss at 11999: 0.819216605653055\n",
      "loss at 1999: 0.7926845996007323\n",
      "loss at 3999: 0.7811656927168369\n",
      "loss at 5999: 0.8131427071280778\n",
      "loss at 7999: 0.8583107233792543\n",
      "loss at 9999: 0.8298938591573387\n",
      "loss at 11999: 0.7853196448897943\n",
      "loss at 1999: 0.7610678947642445\n",
      "loss at 3999: 0.7498976232036948\n",
      "loss at 5999: 0.7825025436617434\n",
      "loss at 7999: 0.8260377780143171\n",
      "loss at 9999: 0.7987665629573166\n",
      "loss at 11999: 0.7552165974611417\n"
     ]
    }
   ],
   "source": [
    "# more epochs, now measuring time\n",
    "\n",
    "# with a bit from https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "for epoch in range(4):\n",
    "    running_loss = 0.0\n",
    "    for i, tagged_sent in enumerate(train_sents):\n",
    "        # optimizer.zero_grad()\n",
    "        model.zero_grad()  # are these equivalent?\n",
    "\n",
    "        sent, tags = zip(*tagged_sent)\n",
    "        isent = prepare_sequence(sent, v_to_i)\n",
    "        itags = prepare_sequence(tags, t_to_i)\n",
    "\n",
    "        tag_scores = model(isent)  # forwad step\n",
    "        loss = loss_function(tag_scores, itags)\n",
    "        loss.backward()  # calls backwards on tag_scores and then inside all the model\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:\n",
    "            print('loss at {}: {}'.format(i, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1006.1496112346649"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    y_true, y_pred = [], []\n",
    "    for tagged_sent in test_sents:\n",
    "        sent, tags = zip(*tagged_sent)\n",
    "        \n",
    "        y_true.extend(tags)\n",
    "        \n",
    "        seq = prepare_sequence(sent, v_to_i)\n",
    "        tag_scores = model(seq)\n",
    "        tags = tag_scores.argmax(dim=1)\n",
    "        pred_tags = [i_to_t[tag] for tag in tags]\n",
    "        y_pred.extend(pred_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7301969226872665"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# on ~5 epochs\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval():\n",
    "    with torch.no_grad():\n",
    "        y_true, y_pred = [], []\n",
    "        for tagged_sent in test_sents:\n",
    "            sent, tags = zip(*tagged_sent)\n",
    "\n",
    "            y_true.extend(tags)\n",
    "\n",
    "            seq = prepare_sequence(sent, v_to_i)\n",
    "            tag_scores = model(seq)\n",
    "            tags = tag_scores.argmax(dim=1)\n",
    "            pred_tags = [i_to_t[tag] for tag in tags]\n",
    "            y_pred.extend(pred_tags)\n",
    "    \n",
    "    return y_true, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true, y_pred = eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7618776251081703"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Move to CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.cuda.is_available()\n",
    "tag_scores.is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMTagger(\n",
       "  (word_embeddings): Embedding(10000, 10)\n",
       "  (lstm): LSTM(10, 10)\n",
       "  (hidden2tag): Linear(in_features=10, out_features=83, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# move things to cuda\n",
    "model.to(device)  # out of memory :("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODOs\n",
    "\n",
    "- Save on disk!\n",
    "- Gradient clipping?\n",
    "- Regularization?\n",
    "\n",
    "Advanced:\n",
    "\n",
    "- Fixed pre-computed embeddings\n",
    "- Beam search\n",
    "- Bidirectional LSTM\n",
    "- Multi-layer LSTM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
